{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Input Processing with the SpeechToText & VoiceInputChain classess to run on Ollama models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**[SpeechToText](https://openai.com/index/whisper/)** is a wrapper around OpenAI Whisper API which utilizes machine learning to transcribe audio files to english text. \n",
    ">\n",
    ">The Parser supports `.mp3`, `.mp4`, `.mpeg`, `.mpga`, `.m4a`, `.wav`, and `.webm`.\n",
    "\n",
    "The current implementation follows LangChain core principles and can be used with other loaders to handle both audio downloading and parsing. As a result of this the parser will `yield` an `Iterator[Document]`.\n",
    "\n",
    "> **VoiceInputChain** is a class that runs chains based on voice input from users.\n",
    ">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **SpeechToText** class requires an OpenAI api key to function (either passed to the class or as an environment variable), while the **VoiceInputChain** class does not require a key (due to the nature of ollama models). Ollama and the preferred model being used (on Ollama) should be downloaded on the user's device. Furthermore, the required dependencies must also be installed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq langchain langchain-community openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 1: Using pre-recorded audio as voice input for Ollama models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `speechToText`'s method, `.lazy_parse`, accepts a `Blob` object as a parameter containing the file path of the file to be transcribed. Once transcribed, audio input can be fed into the `VoiceInputChain` class to be ran through an Ollama model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"path/to/your/audio/file\"\n",
    "key = \"<your_api_key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.ollama_voice_input import SpeechToText, VoiceInputChain\n",
    "\n",
    "stt = SpeechToText(api_key=key, audio_path=audio_path)\n",
    "voice_model = VoiceInputChain(stt=stt)  # llama2 model by default\n",
    "response = voice_model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)  # view response from voice input from Ollama model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 2: Recording audio with 'SpeechToText' object as voice input for Ollama models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the `speechToText`'s method, `.record_audio` accepts `duration` & `sample_rate` integer parameters (in seconds) to record voice input from the user, and saves it at parameter `path`. The audio input will be saved as `audio_input.wav` in the current directory.\n",
    "Then, the `speechToText`'s method, `.lazy_parse`, accepts a `Blob` object as a parameter containing the file path of the file to be transcribed. Once transcribed, audio input can be fed into the `VoiceInputChain` class to be ran through an Ollama model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.ollama_voice_input import SpeechToText, VoiceInputChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stt = SpeechToText(api_key=key)\n",
    "stt.record_audio(duration=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_model = VoiceInputChain(stt=stt)  # llama2 model by default\n",
    "response = voice_model.run()\n",
    "print(response)  # view response from voice input from Ollama model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 3: Using pre-recorded voice input for Ollama model chains that are uniquely customised beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `speechToText`'s method, `.lazy_parse`, accepts a `Blob` object as a parameter containing the file path of the file to be transcribed. Once transcribed, audio input can be fed into the `VoiceInputChain` class to be ran through an pre-made Ollama chain (e.g a custom summarizer for RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import ollama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"\"\"You are an assistant tasked with summarizing text for retrieval.\n",
    "These summaries will be embedded and used to retrieve the raw text.\n",
    "Give a concise summary of the or text that is well optimized for retrieval. Text: {element}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "model = ollama.Ollama(temperature=0, model=\"llama2\", api_key=key)\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stt = SpeechToText(api_key=key)\n",
    "voice_model = VoiceInputChain(stt=stt, chain=summarize_chain)\n",
    "print(voice_model.run())  # view response from voice input from Ollama chain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
